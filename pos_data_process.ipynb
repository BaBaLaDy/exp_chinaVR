{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyemd'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 11\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdecomposition\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PCA\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mglob\u001B[39;00m\n\u001B[1;32m---> 11\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mCalculate_Feature\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mspline\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m np_move_avg\n",
      "File \u001B[1;32mD:\\MyWork\\generated IMU\\code\\exp_generate\\Calculate_Feature.py:15\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmath\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# import pywt\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyemd\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m EMD\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mnp_move_avg\u001B[39m(a,n,mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msame\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m     20\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m(np\u001B[38;5;241m.\u001B[39mconvolve(a, np\u001B[38;5;241m.\u001B[39mones((n,))\u001B[38;5;241m/\u001B[39mn, mode\u001B[38;5;241m=\u001B[39mmode))\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'pyemd'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from merge_script import modify_first_column\n",
    "import math\n",
    "from sklearn.decomposition import PCA\n",
    "import glob\n",
    "import Calculate_Feature\n",
    "import os\n",
    "from spline import np_move_avg\n",
    "\n",
    "def use_pca(featuer_data, n_components=5):\n",
    "    pca = PCA(n_components)\n",
    "    axis_fea = pca.fit_transform(featuer_data)  # 每个样本降为n_components维\n",
    "    fea = []\n",
    "    for raw in range(axis_fea.shape[0]):\n",
    "        for ele in axis_fea[raw, :]:\n",
    "            fea.append(ele)\n",
    "    return fea\n",
    "\n",
    "\n",
    "def axis_normalization(datalist, stage=1):\n",
    "    normalized_data = []\n",
    "    #datalist = np.array(datalist)\n",
    "    for data in datalist:\n",
    "        maximum = max(data)\n",
    "        minimum = min(data)\n",
    "        # print(\"maximum is:\",maximum,\"minimum is:\",minimum)\n",
    "        normalized_data.append([(item - minimum) / (maximum - minimum) * stage for item in data])\n",
    "    return np.array(normalized_data)\n",
    "\n",
    "\n",
    "def three_fuse(data):\n",
    "    axis_num = int(data.shape[0] / 3)\n",
    "    f_nd = []\n",
    "\n",
    "    for f_in in range(axis_num):\n",
    "        nd = []\n",
    "        index = f_in * 3\n",
    "        for i in range(data.shape[1]):\n",
    "            d = data[:, i]\n",
    "            fuse_data = math.sqrt(d[index] * d[index] + d[index + 1] * d[index + 1] + d[index + 2] * d[index + 2])\n",
    "            nd.append(fuse_data)\n",
    "        f_nd.append(nd)\n",
    "    return f_nd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-05T16:05:43.751917700Z",
     "start_time": "2024-07-05T16:05:41.440084400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['save_data/Animation_record/pos\\\\ankletap_m_pos_merge_data.csv']\n",
      "['save_data/Animation_record/pos\\\\highKnee_m_pos_merge_data.csv']\n",
      "['save_data/Animation_record/pos\\\\Kneekick_m_pos_merge_data.csv']\n",
      "['save_data/Animation_record/pos\\\\reverseLunge_m_pos_merge_data.csv']\n",
      "['save_data/Animation_record/pos\\\\sideCrunch_m_pos_merge_data.csv']\n",
      "['save_data/Animation_record/pos\\\\sidetoside_m_pos_merge_data.csv']\n",
      "['save_data/Animation_record/pos\\\\warmup_m_pos_merge_data.csv']\n"
     ]
    }
   ],
   "source": [
    "# target_path = \"save_data/Vitural_NewData/scale_motion_0.8merge/pos/\"\n",
    "target_path = \"save_data/Animation_record/pos/\"\n",
    "motion_name = ['ankle', 'highKnee', 'Kneekick', 'reverseLunge', 'sideCrunch', 'sidetoside', 'warmup']\n",
    "\n",
    "for motion in motion_name:\n",
    "    # pos_save_path = 'save_data/frompos_NewVirtualData/0.8merge/' + motion + '/'\n",
    "    pos_save_path = 'save_data/Animation_record_pos_data/' + motion + '/'\n",
    "    if not os.path.exists(pos_save_path):  # 判断所在目录下是否有该文件名的文件夹\n",
    "        os.makedirs(pos_save_path)\n",
    "    virtual_spline_data_path = glob.glob(target_path + '*' + motion + '*' + '*.csv')\n",
    "    print(virtual_spline_data_path)\n",
    "    temp_data = []\n",
    "    for file in virtual_spline_data_path:\n",
    "        # test_data_path = 'save_data/Vitural_NewData/scale_motion_1.1merge/pos/ReverseLunge_m_pos_merge_data.csv'\n",
    "        # test_data_path = 'save_data/Vitural_NewData/scale_motion_1.1merge/pos/ankle_m_pos_merge_data.csv'\n",
    "        modify_first_column(file)\n",
    "        df = pd.read_csv(file,header=None, index_col=False)\n",
    "        df = np.array(df).T\n",
    "        time_list = df[0, :]\n",
    "        for index in range(df.shape[0]-2):\n",
    "        # for index in range(data.shape[0]):\n",
    "            # x = data[:,index+2]\n",
    "            x = df[index+2, :]\n",
    "            # print(x)\n",
    "            # print(x)\n",
    "            second_pos = spline.spline_cal(time_list, x.tolist(), 1)\n",
    "            re_x = second_pos.second_derivate()\n",
    "\n",
    "            temp_data.append(re_x[50:len(re_x) - 50])  # 去掉头尾\n",
    "        temp_data = np.array(temp_data)\n",
    "        write_path = pos_save_path + motion + \".csv\"\n",
    "        with open(write_path, mode=\"w\", encoding=\"utf-8\", newline='') as f:\n",
    "            csv_writer = csv.writer(f)\n",
    "            for col in range(temp_data.shape[1]):\n",
    "                write_data = temp_data[:, col]\n",
    "                csv_writer.writerow(write_data)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T02:52:42.904714500Z",
     "start_time": "2023-12-20T02:52:42.540549400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_data/Virtual_New_New_xy_18/scale_motion_0.5merge/pos/\n",
      "['save_data/Virtual_New_New_xy_18/scale_motion_0.5merge/pos\\\\ankle_m_pos_merge_data.csv']\n",
      "['save_data/Virtual_New_New_xy_18/scale_motion_0.5merge/pos\\\\highKnee_m_pos_merge_data.csv']\n",
      "['save_data/Virtual_New_New_xy_18/scale_motion_0.5merge/pos\\\\reverseLunge_m_pos_merge_data.csv']\n",
      "['save_data/Virtual_New_New_xy_18/scale_motion_0.5merge/pos\\\\sideCrunch_m_pos_merge_data.csv']\n",
      "['save_data/Virtual_New_New_xy_18/scale_motion_0.5merge/pos\\\\sidetoside_m_pos_merge_data.csv']\n",
      "['save_data/Virtual_New_New_xy_18/scale_motion_0.5merge/pos\\\\warmup_m_pos_merge_data.csv']\n",
      "save_data/Virtual_New_New_xy_18/scale_motion_0.7merge/pos/\n",
      "['save_data/Virtual_New_New_xy_18/scale_motion_0.7merge/pos\\\\ankle_m_pos_merge_data.csv']\n",
      "['save_data/Virtual_New_New_xy_18/scale_motion_0.7merge/pos\\\\highKnee_m_pos_merge_data.csv']\n",
      "['save_data/Virtual_New_New_xy_18/scale_motion_0.7merge/pos\\\\reverseLunge_m_pos_merge_data.csv']\n",
      "['save_data/Virtual_New_New_xy_18/scale_motion_0.7merge/pos\\\\sideCrunch_m_pos_merge_data.csv']\n",
      "['save_data/Virtual_New_New_xy_18/scale_motion_0.7merge/pos\\\\sidetoside_m_pos_merge_data.csv']\n",
      "['save_data/Virtual_New_New_xy_18/scale_motion_0.7merge/pos\\\\warmup_m_pos_merge_data.csv']\n",
      "save_data/Virtual_New_New_xy_18/scale_motion_0.9merge/pos/\n",
      "['save_data/Virtual_New_New_xy_18/scale_motion_0.9merge/pos\\\\ankle_m_pos_merge_data.csv']\n",
      "['save_data/Virtual_New_New_xy_18/scale_motion_0.9merge/pos\\\\highKnee_m_pos_merge_data.csv']\n",
      "['save_data/Virtual_New_New_xy_18/scale_motion_0.9merge/pos\\\\reverseLunge_m_pos_merge_data.csv']\n",
      "['save_data/Virtual_New_New_xy_18/scale_motion_0.9merge/pos\\\\sideCrunch_m_pos_merge_data.csv']\n",
      "['save_data/Virtual_New_New_xy_18/scale_motion_0.9merge/pos\\\\sidetoside_m_pos_merge_data.csv']\n",
      "['save_data/Virtual_New_New_xy_18/scale_motion_0.9merge/pos\\\\warmup_m_pos_merge_data.csv']\n",
      "save_data/Virtual_New_New_xy_18/scale_motion_1.1merge/pos/\n",
      "['save_data/Virtual_New_New_xy_18/scale_motion_1.1merge/pos\\\\ankle_m_pos_merge_data.csv']\n",
      "['save_data/Virtual_New_New_xy_18/scale_motion_1.1merge/pos\\\\highKnee_m_pos_merge_data.csv']\n",
      "['save_data/Virtual_New_New_xy_18/scale_motion_1.1merge/pos\\\\reverseLunge_m_pos_merge_data.csv']\n",
      "['save_data/Virtual_New_New_xy_18/scale_motion_1.1merge/pos\\\\sideCrunch_m_pos_merge_data.csv']\n",
      "['save_data/Virtual_New_New_xy_18/scale_motion_1.1merge/pos\\\\sidetoside_m_pos_merge_data.csv']\n",
      "['save_data/Virtual_New_New_xy_18/scale_motion_1.1merge/pos\\\\warmup_m_pos_merge_data.csv']\n",
      "save_data/Virtual_New_New_xy_18/scale_motion_1.3merge/pos/\n",
      "['save_data/Virtual_New_New_xy_18/scale_motion_1.3merge/pos\\\\ankle_m_pos_merge_data.csv']\n",
      "['save_data/Virtual_New_New_xy_18/scale_motion_1.3merge/pos\\\\highKnee_m_pos_merge_data.csv']\n",
      "['save_data/Virtual_New_New_xy_18/scale_motion_1.3merge/pos\\\\reverseLunge_m_pos_merge_data.csv']\n",
      "['save_data/Virtual_New_New_xy_18/scale_motion_1.3merge/pos\\\\sideCrunch_m_pos_merge_data.csv']\n",
      "['save_data/Virtual_New_New_xy_18/scale_motion_1.3merge/pos\\\\sidetoside_m_pos_merge_data.csv']\n",
      "['save_data/Virtual_New_New_xy_18/scale_motion_1.3merge/pos\\\\warmup_m_pos_merge_data.csv']\n",
      "save_data/Virtual_New_New_xy_18/scale_motion_1.5merge/pos/\n",
      "['save_data/Virtual_New_New_xy_18/scale_motion_1.5merge/pos\\\\ankle_m_pos_merge_data.csv']\n",
      "['save_data/Virtual_New_New_xy_18/scale_motion_1.5merge/pos\\\\highKnee_m_pos_merge_data.csv']\n",
      "['save_data/Virtual_New_New_xy_18/scale_motion_1.5merge/pos\\\\reverseLunge_m_pos_merge_data.csv']\n",
      "['save_data/Virtual_New_New_xy_18/scale_motion_1.5merge/pos\\\\sideCrunch_m_pos_merge_data.csv']\n",
      "['save_data/Virtual_New_New_xy_18/scale_motion_1.5merge/pos\\\\sidetoside_m_pos_merge_data.csv']\n",
      "['save_data/Virtual_New_New_xy_18/scale_motion_1.5merge/pos\\\\warmup_m_pos_merge_data.csv']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scale_list = [\"0.5\", \"0.7\", \"0.9\", \"1.1\", \"1.3\", \"1.5\"]\n",
    "for ID in scale_list:\n",
    "    # target_path = \"save_data/Vitural_NewData/scale_motion_0.8merge/pos/\"\n",
    "    # target_path = \"save_data/Vitural_New_New/scale_motion_\" + ID + \"merge/pos/\"\n",
    "    target_path = \"save_data/Virtual_New_New_xy_18/scale_motion_\" + ID + \"merge/pos/\"\n",
    "    print(target_path)\n",
    "    # motion_name = ['ankle', 'highKnee', 'Kneekick', 'reverseLunge', 'sideCrunch', 'sidetoside', 'warmup']\n",
    "    motion_name = ['ankle', 'highKnee', 'reverseLunge', 'sideCrunch', 'sidetoside', 'warmup']\n",
    "    # motion_name = ['reverseLunge']\n",
    "\n",
    "    for motion in motion_name:\n",
    "        # pos_save_path = 'save_data/frompos_NewVirtualData/0.8merge/' + motion + '/'\n",
    "        # pos_save_path = 'save_data/pos_New_New_VirtualData/' + ID + 'merge/' + motion + '/'\n",
    "        pos_save_path = 'save_data/pos_New_New_VirtualData_xy_18/' + ID + 'merge/' + motion + '/'\n",
    "        if not os.path.exists(pos_save_path):  # 判断所在目录下是否有该文件名的文件夹\n",
    "            os.makedirs(pos_save_path)\n",
    "        virtual_spline_data_path = glob.glob(target_path + '*' + motion + '*' + '*.csv')\n",
    "        print(virtual_spline_data_path)\n",
    "        temp_data = []\n",
    "        for file in virtual_spline_data_path:\n",
    "            # test_data_path = 'save_data/Vitural_NewData/scale_motion_1.1merge/pos/ReverseLunge_m_pos_merge_data.csv'\n",
    "            # test_data_path = 'save_data/Vitural_NewData/scale_motion_1.1merge/pos/ankle_m_pos_merge_data.csv'\n",
    "            modify_first_column(file)\n",
    "            df = pd.read_csv(file,header=None, index_col=False)\n",
    "            df = np.array(df).T\n",
    "            time_list = df[0, :]\n",
    "            # for index in range(df.shape[0]-2):\n",
    "            for index in range(df.shape[0]):\n",
    "                # x = data[:,index+2]\n",
    "                x = df[index, :]\n",
    "\n",
    "                second_pos = spline.spline_cal(time_list, x.tolist(), 1)\n",
    "                re_x = second_pos.second_derivate()\n",
    "\n",
    "                temp_data.append(re_x[10:len(re_x) - 10])  # 去掉头尾\n",
    "            temp_data = np.array(temp_data)\n",
    "            write_path = pos_save_path + motion + \".csv\"\n",
    "            with open(write_path, mode=\"w\", encoding=\"utf-8\", newline='') as f:\n",
    "                csv_writer = csv.writer(f)\n",
    "                for col in range(temp_data.shape[1]):\n",
    "                    write_data = temp_data[:, col]\n",
    "                    csv_writer.writerow(write_data)\n",
    "            modify_first_column(write_path)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T07:57:04.131818400Z",
     "start_time": "2024-02-21T07:57:01.740430Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, LeaveOneOut\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import svm, tree\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T07:57:11.097116800Z",
     "start_time": "2024-02-21T07:57:11.035689Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "['save_data/Virtual_New_Conv_x_spline4_240/0', 'save_data/Virtual_New_Conv_x_spline4_240/1', 'save_data/Virtual_New_Conv_x_spline4_240/2', 'save_data/Virtual_New_Conv_x_spline4_240/3', 'save_data/Virtual_New_Conv_1230_spline4_240/0', 'save_data/Virtual_New_Conv_1230_spline4_240/1', 'save_data/Virtual_New_Conv_1230_spline4_240/2', 'save_data/Virtual_New_Conv_1231_spline3_240/0', 'save_data/Virtual_New_Conv_1231_spline3_240/1', 'save_data/Virtual_New_Conv_1231_spline3_240/2', 'save_data/Virtual_New_Conv_1230_spline3_240/0', 'save_data/Virtual_New_Conv_1230_spline3_240/1', 'save_data/Virtual_New_Conv_1230_spline3_240/2', 'save_data/Virtual_New_Conv_1230_spline5_240/0', 'save_data/Virtual_New_Conv_1230_spline5_240/1', 'save_data/Virtual_New_Conv_1230_spline5_240/2', 'save_data/Virtual_New_Conv_1231_spline4_240/0', 'save_data/Virtual_New_Conv_1231_spline4_240/1', 'save_data/Virtual_New_Conv_1231_spline4_240/2', 'save_data/Virtual_New_Conv_1231_spline5_240/0', 'save_data/Virtual_New_Conv_1231_spline5_240/1', 'save_data/Virtual_New_Conv_1231_spline5_240/2', 'save_data/Virtual_New_Conv_0101_spline4_240/0', 'save_data/Virtual_New_Conv_0101_spline4_240/1', 'save_data/Virtual_New_Conv_0101_spline4_240/2', 'save_data/Virtual_New_Conv_0101_spline3_240/0', 'save_data/Virtual_New_Conv_0101_spline3_240/1', 'save_data/Virtual_New_Conv_0101_spline3_240/2', 'save_data/Virtual_New_Conv_0101_spline5_240/0', 'save_data/Virtual_New_Conv_0101_spline5_240/1', 'save_data/Virtual_New_Conv_0101_spline5_240/2', 'save_data/Virtual_New_Conv_0101_2_spline3_240/0', 'save_data/Virtual_New_Conv_0101_2_spline3_240/1', 'save_data/Virtual_New_Conv_0101_2_spline3_240/2', 'save_data/Virtual_New_Conv_0101_2_spline4_240/0', 'save_data/Virtual_New_Conv_0101_2_spline4_240/1', 'save_data/Virtual_New_Conv_0101_2_spline4_240/2', 'save_data/Virtual_New_Conv_0101_2_spline5_240/0', 'save_data/Virtual_New_Conv_0101_2_spline5_240/1', 'save_data/Virtual_New_Conv_0101_2_spline5_240/2', 'save_data/RealData_Seg_byName/Effy', 'save_data/RealData_Seg_byName/Leafy', 'save_data/RealData_Seg_byName/Nick', 'save_data/RealData_Seg_byName/Qin', 'save_data/RealData_Seg_byName/Tonii', 'save_data/RealData_Seg_byName/Xu', 'save_data/RealData_Seg_byName/Yamamoto']\n"
     ]
    }
   ],
   "source": [
    "# l_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "l_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "l_svc = svm.SVC(C=1.0, kernel='rbf')\n",
    "# l_svc = svm.SVC(C=1000, kernel='rbf')\n",
    "l_clftree = tree.DecisionTreeClassifier(criterion='entropy',random_state=42)\n",
    "\n",
    "# loo = LeaveOneOut()\n",
    "\n",
    "real_path = 'save_data/12-16-Realdataset-Xia/Dataset-ID-'\n",
    "\n",
    "ID = [1, 2, 3, 4, 5, 6, 7]\n",
    "label = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "# encode_feauture = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
    "# encode_feauture = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "\n",
    "# Real_data_path = \"save_data/RealData_Seg_byName_120/\"\n",
    "Real_data_path = \"save_data/RealData_Seg_byName/\"\n",
    "\n",
    "# virtual_data_path = \"save_data/Virtual_Seg_byScale_spline/\"\n",
    "# virtual_data_path = \"save_data/Virtual_Seg_byScale/\"\n",
    "# virtual_data_path = \"save_data/Pos_New_New_Virtual_Seg_byScale_spline/\" # 新\n",
    "\n",
    "# virtual_data_path_3 = \"save_data/Pos_New_New_Virtual_Seg_byScale_spline5_120/\"  #nice\n",
    "# virtual_data_path = \"save_data/Pos_New_New_Virtual_Seg_byScale_xy_60/\" # 最新\n",
    "\n",
    "virtual_data_path = \"save_data/Pos_New_New_Virtual_Seg_byScale_xy_spline5_120/\" # awsome !!!\n",
    "# virtual_data_path_2 = \"save_data/Pos_New_New_Virtual_Seg_byScale_xy_18_spline3_60/\" # awsome !!!\n",
    "\n",
    "\n",
    "# virtual_data_path_2 = \"save_data/Pos_New_New_VirtualData_addPelvis_xyz_25_spline5_120/\"\n",
    "\n",
    "ori_data_path_2 = \"save_data/Animation_record_pos_data_spline5_120\"\n",
    "# ori_data_path_2 = \"save_data/Ori_motion_pos_data_spline5_120\"\n",
    "\n",
    "ori_data_path = 'save_data/Conv_Motion_acc_data_spline3_240/'\n",
    "\n",
    "\n",
    "# virtual_data_path = \"save_data/Pos_New_New_Virtual_Seg_byScale_xy_120/\" # 最新\n",
    "# virtual_data_path_3 = \"save_data/Pos_New_New_Virtual_Seg_byScale_spline5_120/\"\n",
    "# virtual_data_path_3 = \"save_data/Virtual_Seg_byScale_spline/\"\n",
    "# virtual_data_path = \"save_data/Pos_New_New_Virtual_Seg_byScale_xyz_25_spline5_120/\"\n",
    "# virtual_data_path_3 = \"save_data/Pos_New_New_VirtualData_addPelvis_xyz_25_spline3_240/\"\n",
    "virtual_data_path_2 = \"save_data/Virtual_New_Conv_1230_spline4_240/\"\n",
    "virtual_data_path_6 = \"save_data/Virtual_New_Conv_1230_spline3_240/\"\n",
    "virtual_data_path_7 = \"save_data/Virtual_New_Conv_1230_spline5_240/\"\n",
    "\n",
    "virtual_data_path_5 = \"save_data/Virtual_New_Conv_1231_spline3_240/\"\n",
    "virtual_data_path_8 = \"save_data/Virtual_New_Conv_1231_spline4_240/\"\n",
    "virtual_data_path_9 = \"save_data/Virtual_New_Conv_1231_spline5_240/\"\n",
    "\n",
    "virtual_data_path_10 = \"save_data/Virtual_New_Conv_0101_spline4_240/\"\n",
    "virtual_data_path_11 = \"save_data/Virtual_New_Conv_0101_spline3_240/\"\n",
    "virtual_data_path_12 = \"save_data/Virtual_New_Conv_0101_spline5_240/\"\n",
    "\n",
    "virtual_data_path_13 = \"save_data/Virtual_New_Conv_0101_2_spline3_240/\"\n",
    "virtual_data_path_14 = \"save_data/Virtual_New_Conv_0101_2_spline4_240/\"\n",
    "virtual_data_path_15 = \"save_data/Virtual_New_Conv_0101_2_spline5_240/\"\n",
    "\n",
    "\n",
    "virtual_data_path_3 = \"save_data/Virtual_New_Conv_x_spline4_240/\"\n",
    "# virtual_data_path_4 = \"save_data/Virtual_New_Conv_x_spline3_240/\"\n",
    "virtual_data_path_4 = \"save_data/Virtual_New_Conv_notrans_spline3_240/\"\n",
    "\n",
    "virtual_motion_type = ['ankleTap', 'highKnee', 'reverseLunge', 'sideCrunch', 'sidetoside']\n",
    "\n",
    "motion_name = ['ankle', 'highKnee', 'Kneekick', 'reverseLunge', 'sideCrunch', 'sidetoside', 'warmup']\n",
    "\n",
    "human_name = [\"Effy\", \"Leafy\", \"Nick\", \"Qin\", \"Tonii\", \"Xu\", \"Yamamoto\"]\n",
    "# human_name = [\"Effy\", \"Leafy\", \"Nick\", \"Qin\", \"Xu\", \"Yamamoto\"]\n",
    "# scale_list = [\"0.8\", \"0.9\", \"1.1\",\"1.3\", \"1.2\"]\n",
    "# scale_list = [\"0.5\", \"0.7\", \"0.8\",\"0.9\", \"1.1\", \"1.2\",\"1.3\", \"1.5\"]\n",
    "scale_list = [\"0.5\", \"0.7\", \"0.9\", \"1.1\", \"1.3\", \"1.5\"]\n",
    "# scale_list = [\"0.7\", \"0.9\", \"1.1\", \"1.3\", \"1.5\"]\n",
    "scale_list_conv = [\"0\", \"1\", \"2\", \"3\"]\n",
    "scale_list_conv_2 = [\"0\", \"1\", \"2\"]\n",
    "\n",
    "virtual_total_file = []\n",
    "real_total_file = []\n",
    "total_file = []\n",
    "\n",
    "# for scale in scale_list:\n",
    "#     virtual_total_file.append(virtual_data_path + scale)\n",
    "#     total_file.append(virtual_data_path + scale)\n",
    "\n",
    "for scale in scale_list_conv:\n",
    "    virtual_total_file.append(virtual_data_path_3 + scale)\n",
    "    total_file.append(virtual_data_path_3 + scale)\n",
    "#\n",
    "# for scale in scale_list_conv:\n",
    "#     virtual_total_file.append(virtual_data_path_4 + scale)\n",
    "#     total_file.append(virtual_data_path_4 + scale)\n",
    "\n",
    "# total_file.append(ori_data_path)\n",
    "# total_file.append(ori_data_path)\n",
    "for scale in scale_list_conv_2:\n",
    "    virtual_total_file.append(virtual_data_path_2 + scale)\n",
    "    total_file.append(virtual_data_path_2 + scale)\n",
    "\n",
    "for scale in scale_list_conv_2:\n",
    "    virtual_total_file.append(virtual_data_path_5 + scale)\n",
    "    total_file.append(virtual_data_path_5 + scale)\n",
    "\n",
    "\n",
    "for scale in scale_list_conv_2:\n",
    "    virtual_total_file.append(virtual_data_path_6 + scale)\n",
    "    total_file.append(virtual_data_path_6 + scale)\n",
    "\n",
    "for scale in scale_list_conv_2:\n",
    "    virtual_total_file.append(virtual_data_path_7 + scale)\n",
    "    total_file.append(virtual_data_path_7 + scale)\n",
    "\n",
    "for scale in scale_list_conv_2:\n",
    "    virtual_total_file.append(virtual_data_path_8 + scale)\n",
    "    total_file.append(virtual_data_path_8 + scale)\n",
    "\n",
    "for scale in scale_list_conv_2:\n",
    "    virtual_total_file.append(virtual_data_path_9 + scale)\n",
    "    total_file.append(virtual_data_path_9 + scale)\n",
    "\n",
    "for scale in scale_list_conv_2:\n",
    "    virtual_total_file.append(virtual_data_path_10 + scale)\n",
    "    total_file.append(virtual_data_path_10 + scale)\n",
    "for scale in scale_list_conv_2:\n",
    "    virtual_total_file.append(virtual_data_path_11 + scale)\n",
    "    total_file.append(virtual_data_path_11 + scale)\n",
    "\n",
    "for scale in scale_list_conv_2:\n",
    "    virtual_total_file.append(virtual_data_path_12 + scale)\n",
    "    total_file.append(virtual_data_path_12 + scale)\n",
    "\n",
    "for scale in scale_list_conv_2:\n",
    "    virtual_total_file.append(virtual_data_path_13 + scale)\n",
    "    total_file.append(virtual_data_path_13 + scale)\n",
    "\n",
    "for scale in scale_list_conv_2:\n",
    "    virtual_total_file.append(virtual_data_path_14 + scale)\n",
    "    total_file.append(virtual_data_path_14 + scale)\n",
    "\n",
    "for scale in scale_list_conv_2:\n",
    "    virtual_total_file.append(virtual_data_path_15 + scale)\n",
    "    total_file.append(virtual_data_path_15 + scale)\n",
    "\n",
    "virtual_data_path_scale_t = [\"CorData-Seg/scale_0123_trans_spline3_240/\", \"CorData-Seg/scale_0123_trans_spline4_240/\", \"CorData-Seg/scale_0123_trans_spline5_240/\"]\n",
    "virtual_data_path_scale = [\"CorData-Seg/scale_0123_spline3_240/\", \"CorData-Seg/scale_0123_pline4_240/\", \"CorData-Seg/scale_0123_spline5_240/\"]\n",
    "scale_list_2 = [\"0.7\", \"1.3\"]\n",
    "# for scale in scale_list_2:\n",
    "#     for path in virtual_data_path_scale:\n",
    "#         virtual_total_file.append(path + scale)\n",
    "#         total_file.append(path + scale)\n",
    "# \n",
    "# for scale in scale_list_2:\n",
    "#     for path in virtual_data_path_scale_t:\n",
    "#         virtual_total_file.append(path + scale)\n",
    "#         total_file.append(path + scale)\n",
    "\n",
    "\n",
    "# total_file.append(ori_data_path_2)\n",
    "# for scale in scale_list:\n",
    "#     virtual_total_file.append(virtual_data_path_3 + scale)\n",
    "#     total_file.append(virtual_data_path_3 + scale)\n",
    "\n",
    "# for index in ID:\n",
    "#     real_total_file.append(real_path + str(index))\n",
    "#     total_file.append(real_path + str(index))\n",
    "print(len(total_file))\n",
    "for name in human_name:\n",
    "    real_total_file.append(Real_data_path + name)\n",
    "    total_file.append(Real_data_path + name)\n",
    "print(total_file)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:32:03.179539200Z",
     "start_time": "2024-05-21T14:32:03.167808700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "class CustomLeaveOneOut:\n",
    "    def __init__(self, start=7, end=13):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.current = start\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.current > self.end:\n",
    "            raise StopIteration\n",
    "        test_indices = [self.current]\n",
    "        train_indices = list(range(self.start, self.end + 1))\n",
    "        train_indices.remove(self.current)\n",
    "        self.current += 1\n",
    "        # return list(range(47)) + train_indices, test_indices\n",
    "        # return train_indices, test_indices\n",
    "\n",
    "        # return list(range(8,9))+ list(range(2,6)), test_indices\n",
    "\n",
    "        # return list(range(3,5)) + list(range(1)) + list(range(6,7)), test_indices\n",
    "        # return train_indices, test_indices\n",
    "        return list(range(40)), test_indices\n",
    "\n",
    "\n",
    "\n",
    "# loo = CustomLeaveOneOut(start=6, end=11)\n",
    "#\n",
    "# for train_indices, test_indices in loo:\n",
    "#     print(\"Train Indices:\", train_indices)\n",
    "#     print(\"Test Indices:\", test_indices)\n",
    "\n",
    "# Real_motion_type = ['ankle', 'highKnee', 'Kneekick', 'reverseLunge', 'sideCrunch', 'sidetoside', 'warmup']\n",
    "# Real_motion_type = ['ankle', 'highKnee', 'reverseLunge', 'sideCrunch', 'sidetoside', ]\n",
    "# Real_motion_type = ['ReverseLunge','HighKnee','ankle','Knee']\n",
    "# Real_motion_type = ['ReverseLunge','HighKnee','Warm'] # persent高 目前获得提升 13混淆\n",
    "\n",
    "# Real_motion_type = ['sidetoside','HighKnee','Warm'] # awsome!!!\n",
    "# Real_motion_type = ['sidetoside','HighKnee','ReverseLunge']\n",
    "\n",
    "# Real_motion_type = ['kneekick','reverselunge']\n",
    "\n",
    "# Real_motion_type = ['ReverseLunge','HighKnee','sideCrunch'] # 相同\n",
    "Real_motion_type = ['ReverseLunge','HighKnee','sidetoside'] # 相同awsome!!!!!!!!!!!\n",
    "# Real_motion_type = ['ReverseLunge','HighKnee','sideCrunch'] \n",
    "# Real_motion_type = ['HighKnee','ReverseLunge','Kneekick'] # 相同awsome!!!!!\n",
    "# Real_motion_type = ['ankle','sidetoside','Kneekick'] # 相同awsome!!!!!\n",
    "# Real_motion_type = ['ReverseLunge','ankle','Kneekick'] # persent最高 目前获得提升\n",
    "\n",
    "# Real_motion_type = ['sideCrunch','sidetoside','Ankle'] # awsome!!!\n",
    "# Real_motion_type = ['sideCrunch','sidetoside','warm'] # pca improve!!!\n",
    "\n",
    "# Real_motion_type = ['sideCrunch','sidetoside','ReverseLunge', 'highknee']\n",
    "# Real_motion_type = ['warm','sidetoside','ReverseLunge', 'highknee']\n",
    "# Real_motion_type = ['HighKnee','ReverseLunge','Kneekick'] # persent高 目前获得提升\n",
    "encode_feauture = [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
    "# encode_feauture = [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "# Real_motion_type = ['Warm','HighKnee','sideCrunch','ankle'] # 相同awsome!!!!!\n",
    "# Real_motion_type = ['warm','HighKnee','sidetoside'] # awsome"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:32:17.058749200Z",
     "start_time": "2024-05-21T14:32:17.036739700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39] [40]\n",
      "real sample size is: (2876, 9)\n",
      "save_data/RealData_Seg_byName/Effy\n",
      "test sample size is: (66, 9)\n",
      "*****\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39] [41]\n",
      "real sample size is: (2876, 9)\n",
      "save_data/RealData_Seg_byName/Leafy\n",
      "test sample size is: (66, 9)\n",
      "*****\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39] [42]\n",
      "real sample size is: (2876, 9)\n",
      "save_data/RealData_Seg_byName/Nick\n",
      "test sample size is: (66, 9)\n",
      "*****\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39] [43]\n",
      "real sample size is: (2876, 9)\n",
      "save_data/RealData_Seg_byName/Qin\n",
      "test sample size is: (66, 9)\n",
      "*****\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39] [44]\n",
      "real sample size is: (2876, 9)\n",
      "save_data/RealData_Seg_byName/Tonii\n",
      "test sample size is: (66, 9)\n",
      "*****\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39] [45]\n",
      "real sample size is: (2876, 9)\n",
      "save_data/RealData_Seg_byName/Xu\n",
      "test sample size is: (66, 9)\n",
      "*****\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39] [46]\n",
      "real sample size is: (2876, 9)\n",
      "save_data/RealData_Seg_byName/Yamamoto\n",
      "test sample size is: (66, 9)\n",
      "*****\n"
     ]
    }
   ],
   "source": [
    "# loo = CustomLeaveOneOut(start=1, end=7)\n",
    "# loo = CustomLeaveOneOut(start=7, end=13)\n",
    "# loo = CustomLeaveOneOut(start=14, end=20)\n",
    "# loo = CustomLeaveOneOut(start=8, end=14)\n",
    "loo = CustomLeaveOneOut(start=40, end=46)\n",
    "rf_predicted = []\n",
    "svm_predicted = []\n",
    "decisiontree_predicted = []\n",
    "label_save_list = []\n",
    "predict_SVM_save_list = []\n",
    "predict_RF_save_list = []\n",
    "predict_DT_save_list = []\n",
    "\n",
    "\n",
    "# for train_index, test_index in loo.split(total_file):\n",
    "#     print(train_index,test_index)\n",
    "for train_index, test_index in loo:\n",
    "    print(train_index,test_index)\n",
    "    Data_X = []\n",
    "    Label_X = []\n",
    "    TestData_X = []\n",
    "    TestLabel_X = []\n",
    "    true_label = []\n",
    "    for tmtrain in train_index:\n",
    "        # print(tmtrain)\n",
    "        subject = total_file[tmtrain]  # for each subject\n",
    "        # print(subject)\n",
    "        #subject_file = glob.glob(os.path.join(subject,'*.csv'))\n",
    "        for i in range(len(Real_motion_type)):\n",
    "            motion = Real_motion_type[i]\n",
    "            motion_file = glob.glob(os.path.join(subject + '/' + '*' + motion + '*', '*.csv'))\n",
    "            # motion_file = os.path.join(subject + '/' + 'motion', '*.csv')\n",
    "            # motion_file = glob.glob(os.path.join(subject + '/' + 'motion/', '*.csv'))\n",
    "            # print(subject + '/' +  motion )\n",
    "            # print(motion_file)\n",
    "            for motion_frame in motion_file:\n",
    "                #print(motion_frame)\n",
    "                # df = pd.read_csv(motion_frame, index_col=False, header=None,  usecols= [3,4,5,6,7,8,9,10,11,12,13,14])\n",
    "                # df = pd.read_csv(motion_frame, index_col=False, header=None, usecols= [9,10,11,12,13,14])#1,2,3,4,5,6,7,8,9,10,11,12\n",
    "                df = pd.read_csv(motion_frame, index_col=False, header=None, usecols= [9,10,11])#0,1,2,3,4,5,6,7,8,9,10,11,12\n",
    "                # df = pd.read_csv(motion_frame, index_col=False, header=None, usecols= [9])#0,1,2,3,4,5,6,7,8,9,10,11,12\n",
    "                # df = pd.read_csv(motion_frame, index_col=False, header=None,  usecols= [12,13,14])  #1,2,3,4,5,6,7,8,9,10,11,12\n",
    "                df = np.array(df).T\n",
    "\n",
    "\n",
    "                # timestamp = []\n",
    "                # for t in range(df.shape[1]):\n",
    "                #     timestamp.append(t*0.05)\n",
    "                # timestamp = np.array(timestamp)\n",
    "\n",
    "                # for k in range(df.shape[0]):\n",
    "                #     df[k, :] = np_move_avg(df[k, :], 15, mode='same')\n",
    "\n",
    "                # fuse three axis-data\n",
    "                fuse_df = three_fuse(df)\n",
    "                nor_df = axis_normalization(fuse_df)  #.tolist()\n",
    "                # nor_df = axis_normalization(df)  #.tolist()\n",
    "\n",
    "                axis_fea = []\n",
    "                # rd = []\n",
    "                # print(nor_df.shape)\n",
    "                for raw in range(nor_df.shape[0]):\n",
    "                # for raw in range(df.shape[0]):\n",
    "                    tmd = nor_df[raw, :]\n",
    "\n",
    "                    # resampling = spline.spline_cal(timestamp,tmd.tolist(),10)\n",
    "                    #\n",
    "                    # tmd = resampling.resample()\n",
    "                    # print(tmd)\n",
    "\n",
    "                    # tmd = df[raw, :]\n",
    "                    # rd.append(tmd)\n",
    "                    cal_fea = Calculate_Feature.Get_Feature(tmd, encode_feauture)\n",
    "                    fea = cal_fea.cal_result()\n",
    "                    for f in fea:\n",
    "                        axis_fea.append(f)\n",
    "\n",
    "                # print(axis_fea)\n",
    "\n",
    "                Data_X.append(axis_fea)\n",
    "                Label_X.append(label[i])\n",
    "\n",
    "                # print(Label_X)\n",
    "\n",
    "    Real_Data = np.array(Data_X)\n",
    "    Real_Label = np.array(Label_X)\n",
    "    # print(Real_Data.shape)\n",
    "    # print(Real_Label.shape)\n",
    "\n",
    "    # for i in range(Real_Data.shape[0]):\n",
    "    #     Real_Data[i] = [0 if math.isnan(x) else x for x in Real_Data[i]]\n",
    "    # print(\"real sample size before PCA is:\", Real_Data.shape)\n",
    "    #\n",
    "    # pca = PCA(2)\n",
    "    # Real_Data = pca.fit_transform(Real_Data)\n",
    "\n",
    "    # tsne = TSNE(2)\n",
    "    # Real_Data = tsne.fit_transform(Real_Data)\n",
    "\n",
    "    print(\"real sample size is:\", Real_Data.shape)\n",
    "    l_rf.fit(Real_Data, Real_Label)\n",
    "    l_svc.fit(Real_Data, Real_Label)\n",
    "    l_clftree.fit(Real_Data, Real_Label)\n",
    "\n",
    "    for tmtest in test_index:\n",
    "        # print(tmtest)\n",
    "        subject = total_file[tmtest]  # for each subject\n",
    "        print(subject)\n",
    "        #subject_file = glob.glob(os.path.join(subject,'*.csv'))\n",
    "        for i in range(len(Real_motion_type)):\n",
    "            motion = Real_motion_type[i]\n",
    "            motion_file = glob.glob(os.path.join(subject + '/' + '*' + motion + '*', '*.csv'))\n",
    "            for motion_frame in motion_file:\n",
    "                #print(motion_frame)\n",
    "                # df = pd.read_csv(motion_frame, index_col=False, header=None,  usecols= [3,4,5,6,7,8,9,10,11,12,13,14])  #1,2,3,4,5,6,7,8,9,10,11,12\n",
    "                df = pd.read_csv(motion_frame, index_col=False, header=None,  usecols= [9,10,11])  #1,2,3,4,5,6,7,8,9,10,11,12\n",
    "                # df = pd.read_csv(motion_frame, index_col=False, header=None,  usecols= [9])  #1,2,3,4,5,6,7,8,9,10,11,12\n",
    "                # df = pd.read_csv(motion_frame, index_col=False, header=None,  usecols= [12,13,14])  #1,2,3,4,5,6,7,8,9,10,11,12\n",
    "                # df = pd.read_csv(motion_frame, index_col=False, header=None,  usecols= [9,10,11,12,13,14])  #1,2,3,4,5,6,7,8,9,10,11,12\n",
    "                # df = pd.read_csv(motion_frame, index_col=False, header=None)  #1,2,3,4,5,6,7,8,9,10,11,12\n",
    "                df = np.array(df).T\n",
    "\n",
    "\n",
    "                # timestamp = []\n",
    "                # for t in range(df.shape[1]):\n",
    "                #     timestamp.append(t*0.05)\n",
    "                # timestamp = np.array(timestamp)\n",
    "\n",
    "                #\n",
    "                # for k in range(df.shape[0]):\n",
    "                #     df[k, :] = np_move_avg(df[k, :], 20, mode='same')\n",
    "\n",
    "                #fuse three axis-data\n",
    "                fuse_df = three_fuse(df)\n",
    "                nor_df = axis_normalization(fuse_df)  #.tolist()\n",
    "                # nor_df = axis_normalization(df)  #.tolist()\n",
    "\n",
    "                axis_fea = []\n",
    "                # rd = []\n",
    "                for raw in range(nor_df.shape[0]):\n",
    "                # for raw in range(df.shape[0]):\n",
    "                    tmd = nor_df[raw, :]\n",
    "                    # tmd = df[raw, :]\n",
    "\n",
    "\n",
    "                    # resampling = spline.spline_cal(timestamp,tmd.tolist(),10)\n",
    "                    #\n",
    "                    # tmd = resampling.resample()\n",
    "\n",
    "\n",
    "                    # rd.append(tmd)\n",
    "                    cal_fea = Calculate_Feature.Get_Feature(tmd, encode_feauture)\n",
    "                    fea = cal_fea.cal_result()\n",
    "                    for f in fea:\n",
    "                        axis_fea.append(f)\n",
    "\n",
    "\n",
    "\n",
    "                TestData_X.append(axis_fea)\n",
    "                TestLabel_X.append(label[i])\n",
    "\n",
    "\n",
    "    TestData = np.array(TestData_X)\n",
    "\n",
    "    # for i in range(TestData.shape[0]):\n",
    "    #     TestData[i] = [0 if math.isnan(x) else x for x in TestData[i]]\n",
    "\n",
    "    TestLabel = np.array(TestLabel_X)\n",
    "    label_save_list.append(TestLabel)\n",
    "\n",
    "    # print(\"test sample size before PCA is:\", TestData.shape)\n",
    "\n",
    "    # pca = PCA(2)\n",
    "    # TestData = pca.fit_transform(TestData)\n",
    "\n",
    "    # tsne = TSNE(2)\n",
    "    # TestData = tsne.fit_transform(TestData)\n",
    "\n",
    "    print(\"test sample size is:\", TestData.shape)\n",
    "\n",
    "\n",
    "    predict_RF_save_list.append(l_rf.predict(TestData))\n",
    "    predict_SVM_save_list.append(l_svc.predict(TestData))\n",
    "    predict_DT_save_list.append(l_clftree.predict(TestData))\n",
    "\n",
    "\n",
    "    rf_predicted.append(accuracy_score(TestLabel, l_rf.predict(TestData)))\n",
    "    svm_predicted.append(accuracy_score(TestLabel, l_svc.predict(TestData)))\n",
    "    decisiontree_predicted.append(accuracy_score(TestLabel, l_clftree.predict(TestData)))\n",
    "\n",
    "    #true_label.append(TestLabel_X[0])\n",
    "\n",
    "    print(\"*****\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:38:42.522262800Z",
     "start_time": "2024-05-21T14:37:17.391185200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9393939393939394, 0.6666666666666666, 0.6060606060606061, 0.6212121212121212, 0.48484848484848486, 0.48484848484848486, 0.9242424242424242]\n",
      "[0.9242424242424242, 0.6666666666666666, 0.6515151515151515, 0.6666666666666666, 0.4393939393939394, 0.4696969696969697, 0.9545454545454546]\n",
      "[0.9242424242424242, 0.6060606060606061, 0.5909090909090909, 0.6515151515151515, 0.48484848484848486, 0.5303030303030303, 0.9242424242424242]\n",
      "0.6753246753246752\n",
      "0.6818181818181818\n",
      "0.6731601731601732\n"
     ]
    }
   ],
   "source": [
    "print(rf_predicted)\n",
    "print(svm_predicted)\n",
    "print(decisiontree_predicted)\n",
    "print(sum(rf_predicted) / len(rf_predicted))\n",
    "print(sum(svm_predicted) / len(svm_predicted))\n",
    "print(sum(decisiontree_predicted) / len(decisiontree_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:38:55.362180400Z",
     "start_time": "2024-05-21T14:38:55.344338800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3-5\n",
    "0.7164502164502163\n",
    "0.721861471861472\n",
    "0.6536796536796537\n",
    "\n",
    "3-6\n",
    "0.6774891774891775\n",
    "0.6785714285714286\n",
    "0.6006493506493508\n",
    "\n",
    "2-5\n",
    "0.7608225108225108\n",
    "0.7435064935064934\n",
    "0.6958874458874459\n",
    "\n",
    "4-6\n",
    "0.6590909090909091\n",
    "0.6374458874458874\n",
    "0.5984848484848485\n",
    "\n",
    "0 3-5\n",
    "0.7727272727272727\n",
    "0.7662337662337663\n",
    "0.7132034632034632\n",
    "\n",
    "0 1 3-5\n",
    "0.7694805194805194\n",
    "0.7683982683982684\n",
    "0.7370129870129871\n",
    "\n",
    "0 - 5\n",
    "0.7510822510822511\n",
    "0.7532467532467534\n",
    "0.7045454545454547\n",
    "0 - 6\n",
    "0.7359307359307359\n",
    "0.7683982683982683\n",
    "0.7056277056277056\n",
    "\n",
    "only 3\n",
    "0.7521645021645021\n",
    "0.7597402597402597\n",
    "0.6818181818181819\n",
    "\n",
    "only 4\n",
    "0.7067099567099567\n",
    "0.6547619047619048\n",
    "0.6883116883116883\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF: \n",
      "[[ 27.  30.  97.]\n",
      " [ 15. 117.  22.]\n",
      " [ 73.  36.  45.]]\n",
      "SVM: \n",
      "[[ 97.  37.  20.]\n",
      " [ 15. 114.  25.]\n",
      " [ 32.  55.  67.]]\n",
      "DT: \n",
      "[[ 45.  26.  83.]\n",
      " [ 24. 102.  28.]\n",
      " [ 61.  45.  48.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_RF_list = []\n",
    "confusion_SVM_list = []\n",
    "confusion_DT_list = []\n",
    "# total_confusion_matrix = np.zeros((len(Real_motion_type), len(Real_motion_type)))\n",
    "for i in range(len(label_save_list)):\n",
    "    confusion= confusion_matrix(label_save_list[i], predict_RF_save_list[i])\n",
    "    confusion_RF_list.append(confusion)\n",
    "\n",
    "    confusion= confusion_matrix(label_save_list[i], predict_SVM_save_list[i])\n",
    "    confusion_SVM_list.append(confusion)\n",
    "\n",
    "    confusion= confusion_matrix(label_save_list[i], predict_DT_save_list[i])\n",
    "    confusion_DT_list.append(confusion)\n",
    "    # print(confusion)\n",
    "\n",
    "total_confusion_matrix = np.zeros((len(Real_motion_type), len(Real_motion_type)))\n",
    "for cm in confusion_RF_list:\n",
    "    total_confusion_matrix += cm\n",
    "print(\"RF: \")\n",
    "print(total_confusion_matrix)\n",
    "\n",
    "total_confusion_matrix = np.zeros((len(Real_motion_type), len(Real_motion_type)))\n",
    "for cm in confusion_SVM_list:\n",
    "    total_confusion_matrix += cm\n",
    "print(\"SVM: \")\n",
    "print(total_confusion_matrix)\n",
    "\n",
    "total_confusion_matrix = np.zeros((len(Real_motion_type), len(Real_motion_type)))\n",
    "for cm in confusion_DT_list:\n",
    "    total_confusion_matrix += cm\n",
    "print(\"DT: \")\n",
    "print(total_confusion_matrix)\n",
    "\n",
    "#\n",
    "# plt.scatter(TestData[:, 0], TestData[:, 1])\n",
    "# plt.title('t-SNE Visualization')\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T13:51:06.209939400Z",
     "start_time": "2024-01-03T13:51:06.186614200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] [1]\n",
      "real sample size is: (34, 20)\n",
      "save_data/RealData_Seg_byName/Effy\n",
      "test sample size is: (44, 20)\n",
      "*****\n",
      "[0] [2]\n",
      "real sample size is: (34, 20)\n",
      "save_data/RealData_Seg_byName/Leafy\n",
      "test sample size is: (44, 20)\n",
      "*****\n",
      "[0] [3]\n",
      "real sample size is: (34, 20)\n",
      "save_data/RealData_Seg_byName/Nick\n",
      "test sample size is: (44, 20)\n",
      "*****\n",
      "[0] [4]\n",
      "real sample size is: (34, 20)\n",
      "save_data/RealData_Seg_byName/Qin\n",
      "test sample size is: (44, 20)\n",
      "*****\n",
      "[0] [5]\n",
      "real sample size is: (34, 20)\n",
      "save_data/RealData_Seg_byName/Tonii\n",
      "test sample size is: (44, 20)\n",
      "*****\n",
      "[0] [6]\n",
      "real sample size is: (34, 20)\n",
      "save_data/RealData_Seg_byName/Xu\n",
      "test sample size is: (44, 20)\n",
      "*****\n",
      "[0] [7]\n",
      "real sample size is: (34, 20)\n",
      "save_data/RealData_Seg_byName/Yamamoto\n",
      "test sample size is: (44, 20)\n",
      "*****\n"
     ]
    }
   ],
   "source": [
    "Real_motion_type = ['ankle','sidetoside',]\n",
    "# filter_list = [1,2,3,4]\n",
    "filter_list = [1]\n",
    "loo = CustomLeaveOneOut(start=1, end=7)\n",
    "# loo = CustomLeaveOneOut(start=8, end=14)\n",
    "# loo = CustomLeaveOneOut(start=12, end=18)\n",
    "# loo = CustomLeaveOneOut(start=18, end=24)\n",
    "rf_predicted = []\n",
    "svm_predicted = []\n",
    "decisiontree_predicted = []\n",
    "label_save_list = []\n",
    "predict_SVM_save_list = []\n",
    "predict_RF_save_list = []\n",
    "predict_DT_save_list = []\n",
    "\n",
    "\n",
    "# for train_index, test_index in loo.split(total_file):\n",
    "#     print(train_index,test_index)\n",
    "for train_index, test_index in loo:\n",
    "    print(train_index,test_index)\n",
    "    Data_X = []\n",
    "    Label_X = []\n",
    "    TestData_X = []\n",
    "    TestLabel_X = []\n",
    "    true_label = []\n",
    "    for tmtrain in train_index:\n",
    "        # print(tmtrain)\n",
    "        subject = total_file[tmtrain]  # for each subject\n",
    "        # print(subject)\n",
    "        #subject_file = glob.glob(os.path.join(subject,'*.csv'))\n",
    "        for i in range(len(Real_motion_type)):\n",
    "            motion = Real_motion_type[i]\n",
    "            motion_file = glob.glob(os.path.join(subject + '/' + '*' + motion + '*', '*.csv'))\n",
    "            # motion_file = os.path.join(subject + '/' + 'motion', '*.csv')\n",
    "            # motion_file = glob.glob(os.path.join(subject + '/' + 'motion/', '*.csv'))\n",
    "            # print(subject + '/' +  motion )\n",
    "            # print(motion_file)\n",
    "            for motion_frame in motion_file:\n",
    "                #print(motion_frame)\n",
    "                for filter_num in filter_list:\n",
    "                    # df = pd.read_csv(motion_frame, index_col=False, header=None,  usecols= [3,4,5,6,7,8,9,10,11,12,13,14])\n",
    "                    # df = pd.read_csv(motion_frame, index_col=False, header=None, usecols= [9,10,11,12,13,14])#1,2,3,4,5,6,7,8,9,10,11,12\n",
    "                    # df = pd.read_csv(motion_frame, index_col=False, header=None, usecols= [9,10,11])#0,1,2,3,4,5,6,7,8,9,10,11,12\n",
    "                    df = pd.read_csv(motion_frame, index_col=False, header=None, usecols= [9])#0,1,2,3,4,5,6,7,8,9,10,11,12\n",
    "                    # df = pd.read_csv(motion_frame, index_col=False, header=None,  usecols= [12,13,14])  #1,2,3,4,5,6,7,8,9,10,11,12\n",
    "                    df = np.array(df).T\n",
    "\n",
    "\n",
    "                    # timestamp = []\n",
    "                    # for t in range(df.shape[1]):\n",
    "                    #     timestamp.append(t*0.05)\n",
    "                    # timestamp = np.array(timestamp)\n",
    "\n",
    "                    for k in range(df.shape[0]):\n",
    "                        df[k, :] = np_move_avg(df[k, :], filter_num, mode='same')\n",
    "\n",
    "                    # fuse three axis-data\n",
    "                    # fuse_df = three_fuse(df)\n",
    "                    # nor_df = axis_normalization(fuse_df)  #.tolist()\n",
    "                    nor_df = axis_normalization(df)  #.tolist()\n",
    "\n",
    "                    axis_fea = []\n",
    "                    # rd = []\n",
    "                    # print(nor_df.shape)\n",
    "                    for raw in range(nor_df.shape[0]):\n",
    "                    # for raw in range(df.shape[0]):\n",
    "                        tmd = nor_df[raw, :]\n",
    "\n",
    "                        # resampling = spline.spline_cal(timestamp,tmd.tolist(),10)\n",
    "                        #\n",
    "                        # tmd = resampling.resample()\n",
    "                        # print(tmd)\n",
    "\n",
    "                        # tmd = df[raw, :]\n",
    "                        # rd.append(tmd)\n",
    "                        cal_fea = Calculate_Feature.Get_Feature(tmd, encode_feauture)\n",
    "                        fea = cal_fea.cal_result()\n",
    "                        for f in fea:\n",
    "                            axis_fea.append(f)\n",
    "\n",
    "                    # print(axis_fea)\n",
    "\n",
    "                    Data_X.append(axis_fea)\n",
    "                    Label_X.append(label[i])\n",
    "\n",
    "                # print(Label_X)\n",
    "\n",
    "    Real_Data = np.array(Data_X)\n",
    "    Real_Label = np.array(Label_X)\n",
    "    # print(Real_Data.shape)\n",
    "    # print(Real_Label.shape)\n",
    "\n",
    "    # for i in range(Real_Data.shape[0]):\n",
    "    #     Real_Data[i] = [0 if math.isnan(x) else x for x in Real_Data[i]]\n",
    "    # print(\"real sample size before PCA is:\", Real_Data.shape)\n",
    "    #\n",
    "    # pca = PCA(2)\n",
    "    # Real_Data = pca.fit_transform(Real_Data)\n",
    "\n",
    "    # tsne = TSNE(2)\n",
    "    # Real_Data = tsne.fit_transform(Real_Data)\n",
    "\n",
    "    print(\"real sample size is:\", Real_Data.shape)\n",
    "    l_rf.fit(Real_Data, Real_Label)\n",
    "    l_svc.fit(Real_Data, Real_Label)\n",
    "    l_clftree.fit(Real_Data, Real_Label)\n",
    "\n",
    "    for tmtest in test_index:\n",
    "        # print(tmtest)\n",
    "        subject = total_file[tmtest]  # for each subject\n",
    "        print(subject)\n",
    "        #subject_file = glob.glob(os.path.join(subject,'*.csv'))\n",
    "        for i in range(len(Real_motion_type)):\n",
    "            motion = Real_motion_type[i]\n",
    "            motion_file = glob.glob(os.path.join(subject + '/' + '*' + motion + '*', '*.csv'))\n",
    "            for motion_frame in motion_file:\n",
    "                #print(motion_frame)\n",
    "                # df = pd.read_csv(motion_frame, index_col=False, header=None,  usecols= [3,4,5,6,7,8,9,10,11,12,13,14])  #1,2,3,4,5,6,7,8,9,10,11,12\n",
    "                # df = pd.read_csv(motion_frame, index_col=False, header=None,  usecols= [9,10,11])  #1,2,3,4,5,6,7,8,9,10,11,12\n",
    "                df = pd.read_csv(motion_frame, index_col=False, header=None,  usecols= [9])  #1,2,3,4,5,6,7,8,9,10,11,12\n",
    "                # df = pd.read_csv(motion_frame, index_col=False, header=None,  usecols= [12,13,14])  #1,2,3,4,5,6,7,8,9,10,11,12\n",
    "                # df = pd.read_csv(motion_frame, index_col=False, header=None,  usecols= [9,10,11,12,13,14])  #1,2,3,4,5,6,7,8,9,10,11,12\n",
    "                # df = pd.read_csv(motion_frame, index_col=False, header=None)  #1,2,3,4,5,6,7,8,9,10,11,12\n",
    "                df = np.array(df).T\n",
    "\n",
    "\n",
    "                # timestamp = []\n",
    "                # for t in range(df.shape[1]):\n",
    "                #     timestamp.append(t*0.05)\n",
    "                # timestamp = np.array(timestamp)\n",
    "\n",
    "                #\n",
    "                # for k in range(df.shape[0]):\n",
    "                #     df[k, :] = np_move_avg(df[k, :], 30, mode='same')\n",
    "\n",
    "                #fuse three axis-data\n",
    "                # fuse_df = three_fuse(df)\n",
    "                # nor_df = axis_normalization(fuse_df)  #.tolist()\n",
    "                nor_df = axis_normalization(df)  #.tolist()\n",
    "\n",
    "                axis_fea = []\n",
    "                # rd = []\n",
    "                for raw in range(nor_df.shape[0]):\n",
    "                # for raw in range(df.shape[0]):\n",
    "                    tmd = nor_df[raw, :]\n",
    "                    # tmd = df[raw, :]\n",
    "\n",
    "\n",
    "                    # resampling = spline.spline_cal(timestamp,tmd.tolist(),10)\n",
    "                    #\n",
    "                    # tmd = resampling.resample()\n",
    "\n",
    "\n",
    "                    # rd.append(tmd)\n",
    "                    cal_fea = Calculate_Feature.Get_Feature(tmd, encode_feauture)\n",
    "                    fea = cal_fea.cal_result()\n",
    "                    for f in fea:\n",
    "                        axis_fea.append(f)\n",
    "\n",
    "\n",
    "\n",
    "                TestData_X.append(axis_fea)\n",
    "                TestLabel_X.append(label[i])\n",
    "\n",
    "\n",
    "    TestData = np.array(TestData_X)\n",
    "\n",
    "    # for i in range(TestData.shape[0]):\n",
    "    #     TestData[i] = [0 if math.isnan(x) else x for x in TestData[i]]\n",
    "\n",
    "    TestLabel = np.array(TestLabel_X)\n",
    "    label_save_list.append(TestLabel)\n",
    "\n",
    "    # print(\"test sample size before PCA is:\", TestData.shape)\n",
    "\n",
    "    # pca = PCA(2)\n",
    "    # TestData = pca.fit_transform(TestData)\n",
    "\n",
    "    # tsne = TSNE(2)\n",
    "    # TestData = tsne.fit_transform(TestData)\n",
    "\n",
    "    print(\"test sample size is:\", TestData.shape)\n",
    "\n",
    "\n",
    "    predict_RF_save_list.append(l_rf.predict(TestData))\n",
    "    predict_SVM_save_list.append(l_svc.predict(TestData))\n",
    "    predict_DT_save_list.append(l_clftree.predict(TestData))\n",
    "\n",
    "\n",
    "    rf_predicted.append(accuracy_score(TestLabel, l_rf.predict(TestData)))\n",
    "    svm_predicted.append(accuracy_score(TestLabel, l_svc.predict(TestData)))\n",
    "    decisiontree_predicted.append(accuracy_score(TestLabel, l_clftree.predict(TestData)))\n",
    "\n",
    "    #true_label.append(TestLabel_X[0])\n",
    "\n",
    "    print(\"*****\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T03:33:41.728416700Z",
     "start_time": "2023-12-22T03:33:37.026122Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6818181818181818, 0.5, 0.8636363636363636, 0.5, 0.5, 1.0, 0.5]\n",
      "[0.7727272727272727, 0.8863636363636364, 1.0, 0.5227272727272727, 0.1590909090909091, 1.0, 1.0]\n",
      "[0.5, 0.5, 0.5, 0.5, 0.5, 0.5909090909090909, 0.5]\n",
      "\n",
      "0.6493506493506492\n",
      "0.762987012987013\n",
      "0.512987012987013\n"
     ]
    }
   ],
   "source": [
    "print(rf_predicted)\n",
    "print(svm_predicted)\n",
    "print(decisiontree_predicted)\n",
    "print()\n",
    "print(sum(rf_predicted) / len(rf_predicted))\n",
    "print(sum(svm_predicted) / len(svm_predicted))\n",
    "print(sum(decisiontree_predicted) / len(decisiontree_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T03:33:45.353643100Z",
     "start_time": "2023-12-22T03:33:45.337640500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,3) (2,2) (3,3) ",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-41-04a794c279fe>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[0mtotal_confusion_matrix\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzeros\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mReal_motion_type\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mReal_motion_type\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mcm\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mconfusion_RF_list\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 20\u001B[1;33m     \u001B[0mtotal_confusion_matrix\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mcm\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     21\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"RF: \"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtotal_confusion_matrix\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: operands could not be broadcast together with shapes (3,3) (2,2) (3,3) "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_RF_list = []\n",
    "confusion_SVM_list = []\n",
    "confusion_DT_list = []\n",
    "# total_confusion_matrix = np.zeros((len(Real_motion_type), len(Real_motion_type)))\n",
    "for i in range(len(label_save_list)):\n",
    "    confusion= confusion_matrix(label_save_list[i], predict_RF_save_list[i])\n",
    "    confusion_RF_list.append(confusion)\n",
    "\n",
    "    confusion= confusion_matrix(label_save_list[i], predict_SVM_save_list[i])\n",
    "    confusion_SVM_list.append(confusion)\n",
    "\n",
    "    confusion= confusion_matrix(label_save_list[i], predict_DT_save_list[i])\n",
    "    confusion_DT_list.append(confusion)\n",
    "    # print(confusion)\n",
    "\n",
    "total_confusion_matrix = np.zeros((len(Real_motion_type), len(Real_motion_type)))\n",
    "for cm in confusion_RF_list:\n",
    "    total_confusion_matrix += cm\n",
    "print(\"RF: \")\n",
    "print(total_confusion_matrix)\n",
    "\n",
    "total_confusion_matrix = np.zeros((len(Real_motion_type), len(Real_motion_type)))\n",
    "for cm in confusion_SVM_list:\n",
    "    total_confusion_matrix += cm\n",
    "print(\"SVM: \")\n",
    "print(total_confusion_matrix)\n",
    "\n",
    "total_confusion_matrix = np.zeros((len(Real_motion_type), len(Real_motion_type)))\n",
    "for cm in confusion_DT_list:\n",
    "    total_confusion_matrix += cm\n",
    "print(\"DT: \")\n",
    "print(total_confusion_matrix)\n",
    "\n",
    "#\n",
    "# plt.scatter(TestData[:, 0], TestData[:, 1])\n",
    "# plt.title('t-SNE Visualization')\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-25T01:53:37.570989600Z",
     "start_time": "2023-12-25T01:53:37.512593800Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
